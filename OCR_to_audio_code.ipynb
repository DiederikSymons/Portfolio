{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f7bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image: C:/Users/diede/Downloads/ATE.jpeg\n",
      "Original image size: 1600x1200\n",
      "Cropping to box: (512, 180, 1088, 600)\n",
      "Cropped image size: 576x420\n",
      "Initializing RapidOCR engine...\n",
      "RapidOCR engine initialized.\n",
      "Running OCR on the cropped image...\n",
      "RapidOCR engine initialized.\n",
      "Running OCR on the cropped image...\n",
      "OCR processing finished.\n",
      "\n",
      "--- Combined OCR Prediction ---\n",
      "{'ate': 0.9971916675567627}\n",
      "['ate']\n",
      "OCR processing finished.\n",
      "\n",
      "--- Combined OCR Prediction ---\n",
      "{'ate': 0.9971916675567627}\n",
      "['ate']\n",
      "Matched 'ate' to 'ate'\n",
      "Found audio file: C:/Users/diede/Downloads/ZENBEACH\\Island1.level4.ate.wav\n",
      "Matched 'ate' to 'ate'\n",
      "Found audio file: C:/Users/diede/Downloads/ZENBEACH\\Island1.level4.ate.wav\n",
      "Successfully played audio: C:/Users/diede/Downloads/ZENBEACH\\Island1.level4.ate.wav\n",
      "Successfully played audio: C:/Users/diede/Downloads/ZENBEACH\\Island1.level4.ate.wav\n"
     ]
    }
   ],
   "source": [
    "#PART 1\n",
    "\n",
    "# Import the necessary libraries\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from rapidocr_onnxruntime import RapidOCR\n",
    "\n",
    "# File paths\n",
    "image_path = \"filepath_to_the_screenshot_of_the_game.file_extension\"\n",
    "text_file_path = \"filepath_to_the_textfile_of_all_zenbeach_words/zenbeach.txt\"\n",
    "audio_folder_path = \"filepath_to_the_audiofolder_of_all_zenbeach_audiofiles/zenbeach_folder\"\n",
    "output_dir = \"filepath_to_the_outputfolder_of_the_generated_audiofiles/zenbeach_audio_output\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Image cropping settings\n",
    "CROP_TOP_PERCENT = 0.15    # Percentage to remove from the top, define cropping percentages (0.0 to 1.0)\n",
    "CROP_BOTTOM_PERCENT = 0.50 # Percentage to remove from the bottom\n",
    "CROP_LEFT_PERCENT = 0.32   # Percentage to remove from the left\n",
    "CROP_RIGHT_PERCENT = 0.32  # Percentage to remove from the right\n",
    "\n",
    "# Word processing settings\n",
    "WORD_GAP_THRESHOLD_FACTOR = 0.5 # Factor to determine how close words/parts of words need to be in order to be merged into one prediction\n",
    "SPEECH_RATE = 0.4 # Speech rate for audio generation to inform how fast the audio should be played if generated with TTS\n",
    "\n",
    "# Image Loading and Cropping\n",
    "print(f\"Loading image: {image_path}\")\n",
    "try:\n",
    "    img_pil = Image.open(image_path)\n",
    "    w, h = img_pil.size\n",
    "    print(f\"Original image size: {w}x{h}\")\n",
    "\n",
    "    left = int(w * CROP_LEFT_PERCENT)\n",
    "    top = int(h * CROP_TOP_PERCENT)\n",
    "    right = int(w * (1 - CROP_RIGHT_PERCENT))\n",
    "    bottom = int(h * (1 - CROP_BOTTOM_PERCENT))\n",
    "\n",
    "    if left >= right or top >= bottom:\n",
    "        print(f\"Warning: Invalid crop dimensions ({left},{top},{right},{bottom}). Check percentages.\")\n",
    "        cropped_img_pil = img_pil\n",
    "        print(\"Using original image due to invalid crop.\")\n",
    "    else:\n",
    "        print(f\"Cropping to box: ({left}, {top}, {right}, {bottom})\")\n",
    "        cropped_img_pil = img_pil.crop((left, top, right, bottom))\n",
    "        print(f\"Cropped image size: {cropped_img_pil.width}x{cropped_img_pil.height}\")\n",
    "\n",
    "    cropped_img_np = np.array(cropped_img_pil)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Image file not found at {image_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during image loading or cropping: {e}\")\n",
    "    exit()\n",
    "\n",
    "# OCR Initialization\n",
    "print(\"Initializing RapidOCR engine...\")\n",
    "ocr_engine = RapidOCR()\n",
    "print(\"RapidOCR engine initialized.\")\n",
    "\n",
    "# OCR Prediction\n",
    "print(\"Running OCR on the cropped image...\")\n",
    "results = None\n",
    "try:\n",
    "    results, _ = ocr_engine(cropped_img_np)\n",
    "    print(\"OCR processing finished.\")\n",
    "except NameError:\n",
    "    print(\"Error: Image data (cropped_img_np) is not available. Check loading/cropping steps.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during OCR prediction: {e}\")\n",
    "\n",
    "# Process and Combine Results\n",
    "predictions = []\n",
    "word_confidences = {}\n",
    "\n",
    "if results:\n",
    "    # Sort OCR results by the x-coordinate of the bounding box\n",
    "    sorted_results = sorted(results, key=lambda item: item[0][0][0])\n",
    "\n",
    "    combined_words_data = []\n",
    "    if sorted_results:\n",
    "        # Initialize the first word or parts of a word\n",
    "        current_word_text = sorted_results[0][1]\n",
    "        current_word_confidences = [sorted_results[0][2]]\n",
    "        previous_box = sorted_results[0][0]\n",
    "        previous_box_width = max(p[0] for p in previous_box) - min(p[0] for p in previous_box)\n",
    "        previous_box_right_x = max(p[0] for p in previous_box)\n",
    "\n",
    "        # Iterate through the rest of the words or parts of a word and combine them if they are close enough\n",
    "        for i in range(1, len(sorted_results)):\n",
    "            current_box, current_text, current_confidence = sorted_results[i]\n",
    "\n",
    "            current_box_left_x = min(p[0] for p in current_box)\n",
    "            current_box_width = max(p[0] for p in current_box) - current_box_left_x\n",
    "\n",
    "            gap = current_box_left_x - previous_box_right_x\n",
    "\n",
    "            merge_threshold = WORD_GAP_THRESHOLD_FACTOR * previous_box_width if previous_box_width > 1 else 5\n",
    "\n",
    "            if gap < merge_threshold:\n",
    "                current_word_text += current_text\n",
    "                current_word_confidences.append(current_confidence)\n",
    "            else:\n",
    "                combined_words_data.append((current_word_text, current_word_confidences))\n",
    "                current_word_text = current_text\n",
    "                current_word_confidences = [current_confidence]\n",
    "\n",
    "            previous_box = current_box\n",
    "            previous_box_right_x = max(p[0] for p in previous_box)\n",
    "            previous_box_width = max(p[0] for p in previous_box) - min(p[0] for p in previous_box)\n",
    "\n",
    "        # Add the last word or parts of a word\n",
    "        if current_word_text:\n",
    "            combined_words_data.append((current_word_text, current_word_confidences))\n",
    "\n",
    "    # Collect the new words and calculate their average confidence\n",
    "    for word, confidences in combined_words_data:\n",
    "        avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
    "        predictions.append(word)\n",
    "        word_confidences[word] = avg_confidence\n",
    "\n",
    "else:\n",
    "    print(\"No text detected or an error occurred during OCR.\")\n",
    "\n",
    "# Output\n",
    "print(\"\\n--- Combined OCR Prediction ---\")\n",
    "print(word_confidences) # Dictionary of predicted words and their average confidence\n",
    "print(predictions) # List of predicted words\n",
    "\n",
    "\n",
    "\n",
    "#PART 2\n",
    "\n",
    "import difflib\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import wave\n",
    "import pyaudio\n",
    "import sentencepiece\n",
    "\n",
    "# Function to play audio files\n",
    "def play_audio_file(audio_file_path, chunk=1024):\n",
    "    try:\n",
    "        with wave.open(audio_file_path, \"rb\") as f:\n",
    "            p = pyaudio.PyAudio()\n",
    "            stream = p.open(format=p.get_format_from_width(f.getsampwidth()),\n",
    "                          channels=f.getnchannels(),\n",
    "                          rate=f.getframerate(),\n",
    "                          output=True)\n",
    "            data = f.readframes(chunk)\n",
    "            while data:\n",
    "                stream.write(data)\n",
    "                data = f.readframes(chunk)\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            p.terminate()\n",
    "            print(f\"Successfully played audio: {audio_file_path}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error playing audio {audio_file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Setting up text-to-speech model\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "# Configuring the voice of the model\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "# Single loop to process words and handle audio\n",
    "if predictions:  # Check if there are any predictions\n",
    "    for word in predictions:\n",
    "        try:\n",
    "            # Find the closest match in the text file\n",
    "            audio_path = None\n",
    "            with open(text_file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                closest_match = difflib.get_close_matches(word, [line.split()[1] for line in lines], n=1, cutoff=0.6)\n",
    "                if closest_match:\n",
    "                    matched_word = closest_match[0]\n",
    "                    print(f\"Matched '{word}' to '{matched_word}'\")\n",
    "                    \n",
    "                    # Map the filename for the matched word\n",
    "                    for line in lines:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) == 2:\n",
    "                            filename, word_in_file = parts\n",
    "                            if word_in_file.lower() == matched_word.lower():\n",
    "                                audio_path = os.path.join(audio_folder_path, filename + \".wav\")\n",
    "                                break\n",
    "            \n",
    "            # Try to play existing audio file if found\n",
    "            if audio_path and os.path.exists(audio_path):\n",
    "                print(f\"Found audio file: {audio_path}\")\n",
    "                if play_audio_file(audio_path):\n",
    "                    continue  # Skip TTS generation if audio played successfully\n",
    "            \n",
    "            # If we get here, either no audio file was found or playback failed\n",
    "            # Generate speech using TTS\n",
    "            print(f\"Generating TTS audio for '{word}'...\")\n",
    "            inputs = processor(text=word, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "            speech_np = speech.squeeze().cpu().numpy()\n",
    "            speech_stretched = librosa.effects.time_stretch(speech_np, rate=SPEECH_RATE)\n",
    "            \n",
    "            # Save and play the generated audio\n",
    "            generated_audio_path = os.path.join(output_dir, f\"{word}.wav\")\n",
    "            sf.write(generated_audio_path, speech_stretched, samplerate=16000)\n",
    "            \n",
    "            # Try to play the generated audio\n",
    "            if not play_audio_file(generated_audio_path):\n",
    "                print(f\"Failed to play generated audio for '{word}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing word '{word}': {e}\")\n",
    "else:\n",
    "    print(\"No predictions found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
